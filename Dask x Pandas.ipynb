{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabalhando com Big Data usando Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalando dask\n",
    "#!pip install dask\n",
    "\n",
    "#Link para download do dataframe usado nesta aula (kaggle)\n",
    "# https://www.kaggle.com/datasets/ealtman2019/ibm-transactions-for-anti-money-laundering-aml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando pandas pra que mostre somente float com 2 casas decimais e evite notação científica (1min) 50.000.000.000 5e10\n",
    "pd.options.display.float_format = \"{:,.2f}\".format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo Bigdata com Pandas\n",
    "- Podemos passar o parametro nrows=x para ler somente x linhas em um read_csv\n",
    "- Mas vamos tentar ler toda a base de dados pra ver o que acontece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho = r'C:\\Users\\devda\\Documents\\GitHub\\01-DataFrames\\Big Data financeiro'\n",
    "\n",
    "dfp = pd.read_csv(f'{caminho}\\LI-Large_Trans.csv', nrows=5000) # Chamamos de chunk = pedaço\n",
    "dfp.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazendo análise com pandas por chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas = ['Payment Format', 'Receiving Currency', 'Amount Received']\n",
    "\n",
    "dfp_conc = pd.DataFrame() # dataframe vazio\n",
    "cont = 0\n",
    "\n",
    "for dfp_chunk in pd.read_csv(f'{caminho}\\LI-Large_Trans.csv', chunksize=5000):\n",
    "\n",
    "    dfp_chunk = dfp_chunk.groupby(['Payment Format'])['Amount Received'].sum().rename('total').to_frame().reset_index()\n",
    "    dfp_conc = pd.concat([dfp_conc, dfp_chunk])\n",
    "    display(dfp_chunk)\n",
    "    cont += 1\n",
    "    if cont = 2:\n",
    "        break\n",
    "\n",
    "print(\"---------------------Concatenou-----------------------\")\n",
    "display(dfp_conc)\n",
    "print(\"------------------Agrupou novamente-------------------\")\n",
    "tabela_final = dfp_conc.groupby(['Payment Format'])['total'].sum().rename('total').to_frame().reset_index()\n",
    "display(tabela_final)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objetivo (Primeiro faça com Dask):\n",
    "- Somar todos os  valores recebidos em dolar dos diversos formatos de pagamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separando colunas que iremos trabalhar\n",
    "colunas = ['Payment Format', 'Receiving Currency', 'Amount Received']\n",
    "dfp = dfp[colunas]\n",
    "\n",
    "dfp_dolar = dfp.loc[dfp['Receiving Currency'] == 'US Dollar']\n",
    "\n",
    "dfp_objetivo = dfp_dolar.groupby(['Payment Format', 'Receiving Currency'])['Amount Received'].sum().rename(\"total received\").to_frame().reset_index()\n",
    "\n",
    "dfp_sorted = dfp_objetivo.sort_values(by=[\"total received\"], ascending=True)\n",
    "\n",
    "display(dfp_sorted)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O que é o dask?\n",
    "\n",
    "Dask DataFrame é uma estrutura de dados distribuída e paralelizada para manipulação de dados em grandes conjuntos de dados. O Dask DataFrame é semelhante ao pandas DataFrame, mas com a diferença de que ele é projetado para trabalhar em conjunto com o Dask, um framework de computação paralela.\n",
    "\n",
    "O Dask DataFrame é composto por várias partições, que são fragmentos dos dados que são distribuídos em vários nós ou máquinas em um cluster. Cada partição contém uma seção dos dados, e essas partições são processadas em paralelo para acelerar o processamento de grandes conjuntos de dados.\n",
    "\n",
    "As partições no Dask DataFrame são criadas automaticamente com base no tamanho dos dados e nos recursos do sistema disponíveis. Quanto mais recursos estiverem disponíveis, mais partições serão criadas, o que pode acelerar ainda mais o processamento. No entanto, muitas partições também podem levar a um overhead excessivo, pois há um custo associado à coordenação de muitas tarefas.\n",
    "\n",
    "O Dask DataFrame permite que os usuários trabalhem com grandes conjuntos de dados que não cabem na memória de um único computador. Ele também oferece recursos para manipulação de dados distribuídos, processamento paralelo, cálculo em cluster e escalabilidade, tornando-o uma ferramenta útil para análise de dados em larga escala.\n",
    "\n",
    "Lembre-se de que, ao usar o Dask, é importante escolher a estrutura de dados correta com base no tipo de operação que você deseja realizar. Para trabalhar com arrays distribuídos, use o dask.array, para dataframes distribuídos, use o dask.dataframe e para bags distribuídos, use o dask.bag."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo Bigdata com dask\n",
    "- Agora vamos tentar ler com dask mas usando computer()\n",
    "- Sem compute() vai haver estouro de memoria como no pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(f'{caminho}\\LI-Large_Trans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carrega informações básicas do df (3min)\n",
    "df.persist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etapa inicial: Limpesa dos dados\n",
    "- Tratar dados ausentes (missing values)\n",
    "- Padronizar dados (conversão dos dados)\n",
    "- Verificar consistência dos dados (dados condizentes, negativos, infinitos, etc)\n",
    "- Remover Outliers\n",
    "- Normalização (escala)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos começar verificando missing values\n",
    "- Note que isnull() não vai servir, pois ainda não temos informações sobre os dados, a não ser que carregue todos os dados pra memória com o compute(), mas isso vai levar a estouro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solução\n",
    "- Temos que definir o que queremos (objetivo) e separar somente os dados necessários da tabela, ai sim trabalhamos com aquele conjunto de dado restrito.\n",
    "\n",
    "Objetivo:\n",
    "- Somar todos os  valores recebidos em dolar dos diversos formatos de pagamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separando colunas que iremos trabalhar\n",
    "colunas = ['Payment Format', 'Receiving Currency', 'Amount Received']\n",
    "# criando um df com o necessário\n",
    "df_rec = df[colunas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando nulos (1min)\n",
    "df_rec.isnull().sum().compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não detectamos missing values nas colunas, podemos prosseguir com a análise.\n",
    "As colunas estão com tipos de dados corretos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rec.describe().compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados estão consistentes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agora vamos executar o objetivo:\n",
    "- Somar todos os  valores recebidos em dolar dos diversos formatos de pagamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrando somente dollar (1min)\n",
    "df_dolar = df_rec.loc[df_rec['Receiving Currency'] == 'US Dollar'].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salve o resultado em uma nova tabela assim poderá salva-la evitando nova consulta custosa. (6seg)\n",
    "df_objetivo = df_dolar.groupby(['Payment Format', 'Receiving Currency'])['Amount Received'].sum().sort_values().rename('Total Received Amount').to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_objetivo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBS: ordenar uma tabela so pode ser realizado apos um compute() pois lembre-se que com dask não sabe quem são os dados, somente apos carregar em memória e \"saber\" sobre os dados podemos ordena-lo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
